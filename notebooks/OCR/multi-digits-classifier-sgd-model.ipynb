{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digits classifier\n",
    "\n",
    "Let's build a model that is able to recognize a single digit from 0 to 9 from an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading and setting up dependencies\n",
    "import fastbook\n",
    "\n",
    "fastbook.setup_book()\n",
    "\n",
    "from fastai.vision.all import *\n",
    "from fastbook import *\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/yactouat/learning_AI/notebooks/OCR/multi-digits-classifier-sgd-model.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/yactouat/learning_AI/notebooks/OCR/multi-digits-classifier-sgd-model.ipynb#Y114sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# checking that we\"re using GPU for this one\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/yactouat/learning_AI/notebooks/OCR/multi-digits-classifier-sgd-model.ipynb#Y114sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m torch\u001b[39m.\u001b[39;49mcuda\u001b[39m.\u001b[39;49mget_device_name()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:419\u001b[0m, in \u001b[0;36mget_device_name\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_device_name\u001b[39m(device: Optional[_device_t] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m    408\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Gets the name of a device.\u001b[39;00m\n\u001b[1;32m    409\u001b[0m \n\u001b[1;32m    410\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[39m        str: the name of the device\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 419\u001b[0m     \u001b[39mreturn\u001b[39;00m get_device_properties(device)\u001b[39m.\u001b[39mname\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:449\u001b[0m, in \u001b[0;36mget_device_properties\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_device_properties\u001b[39m(device: _device_t) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m _CudaDeviceProperties:\n\u001b[1;32m    440\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Gets the properties of a device.\u001b[39;00m\n\u001b[1;32m    441\u001b[0m \n\u001b[1;32m    442\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[39m        _CudaDeviceProperties: the properties of the device\u001b[39;00m\n\u001b[1;32m    448\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m     _lazy_init()  \u001b[39m# will define _get_device_properties\u001b[39;00m\n\u001b[1;32m    450\u001b[0m     device \u001b[39m=\u001b[39m _get_device_index(device, optional\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    451\u001b[0m     \u001b[39mif\u001b[39;00m device \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m device \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m device_count():\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:298\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mCUDA_MODULE_LOADING\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39menviron:\n\u001b[1;32m    297\u001b[0m     os\u001b[39m.\u001b[39menviron[\u001b[39m\"\u001b[39m\u001b[39mCUDA_MODULE_LOADING\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mLAZY\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 298\u001b[0m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_cuda_init()\n\u001b[1;32m    299\u001b[0m \u001b[39m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \u001b[39m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \u001b[39m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    302\u001b[0m _tls\u001b[39m.\u001b[39mis_initializing \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver."
     ]
    }
   ],
   "source": [
    "# checking that we\"re using GPU for this one\n",
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2) [Path('testing'),Path('training')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import MNIST dataset\n",
    "path_to_mnist = untar_data(URLs.MNIST)\n",
    "Path.BASE_PATH = path_to_mnist\n",
    "\n",
    "path_to_mnist.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the previous output, this dataset is organized into separate folders for testing and training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((#10) [Path('training/9'),Path('training/1'),Path('training/2'),Path('training/6'),Path('training/3'),Path('training/5'),Path('training/8'),Path('training/4'),Path('training/7'),Path('training/0')],\n",
       " (#6742) [Path('training/1/24904.png'),Path('training/1/32446.png'),Path('training/1/15068.png'),Path('training/1/36030.png'),Path('training/1/9628.png'),Path('training/1/1242.png'),Path('training/1/50292.png'),Path('training/1/35908.png'),Path('training/1/18393.png'),Path('training/1/57761.png')...])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's see what's in the training set\n",
    "(path_to_mnist / 'training').ls(), (path_to_mnist / 'training/1').ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((#10) [Path('testing/9'),Path('testing/1'),Path('testing/2'),Path('testing/6'),Path('testing/3'),Path('testing/5'),Path('testing/8'),Path('testing/4'),Path('testing/7'),Path('testing/0')],\n",
       " (#1135) [Path('testing/1/6527.png'),Path('testing/1/6239.png'),Path('testing/1/1548.png'),Path('testing/1/2529.png'),Path('testing/1/6917.png'),Path('testing/1/9760.png'),Path('testing/1/8586.png'),Path('testing/1/4273.png'),Path('testing/1/2982.png'),Path('testing/1/2674.png')...])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now let's see what's in the testing set\n",
    "(path_to_mnist / 'testing').ls(), (path_to_mnist / 'testing/1').ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the paths to all training digits in a list, and all testing digits in another list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_training_0s = (path_to_mnist / 'training' / '0').ls().sorted()\n",
    "mnist_training_1s = (path_to_mnist / 'training' / '1').ls().sorted()\n",
    "mnist_training_2s = (path_to_mnist / 'training' / '2').ls().sorted()\n",
    "mnist_training_3s = (path_to_mnist / 'training' / '3').ls().sorted()\n",
    "mnist_training_4s = (path_to_mnist / 'training' / '4').ls().sorted()\n",
    "mnist_training_5s = (path_to_mnist / 'training' / '5').ls().sorted()\n",
    "mnist_training_6s = (path_to_mnist / 'training' / '6').ls().sorted()\n",
    "mnist_training_7s = (path_to_mnist / 'training' / '7').ls().sorted()\n",
    "mnist_training_8s = (path_to_mnist / 'training' / '8').ls().sorted()\n",
    "mnist_training_9s = (path_to_mnist / 'training' / '9').ls().sorted()\n",
    "\n",
    "mnist_testing_0s = (path_to_mnist / 'testing' / '0').ls().sorted()\n",
    "mnist_testing_1s = (path_to_mnist / 'testing' / '1').ls().sorted()\n",
    "mnist_testing_2s = (path_to_mnist / 'testing' / '2').ls().sorted()\n",
    "mnist_testing_3s = (path_to_mnist / 'testing' / '3').ls().sorted()\n",
    "mnist_testing_4s = (path_to_mnist / 'testing' / '4').ls().sorted()\n",
    "mnist_testing_5s = (path_to_mnist / 'testing' / '5').ls().sorted()\n",
    "mnist_testing_6s = (path_to_mnist / 'testing' / '6').ls().sorted()\n",
    "mnist_testing_7s = (path_to_mnist / 'testing' / '7').ls().sorted()\n",
    "mnist_testing_8s = (path_to_mnist / 'testing' / '8').ls().sorted()\n",
    "mnist_testing_9s = (path_to_mnist / 'testing' / '9').ls().sorted()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a linear model with SGD to classify digits\n",
    "\n",
    "In this approach, we will need a kind of weight assignmen and a way of improving our classifier based on testing the effectiveness of this assignment.\n",
    "\n",
    "_Stochastic gradient descent_ is a technique used to update the weights of a neural network in order to make it improve at any given task. Its power resides in the fact that it provides a way of finding weight values automatically.\n",
    "\n",
    "Instead of trying to find the similarity between an image and an \"ideal image,\" we could instead look at each individual pixel and come up with a set of weights for each one.\n",
    "\n",
    "For instance, pixels toward the bottom right are not very likely to activate neurons for a 7, so they should have a low weight for a 7, but they are likely to be activated for an 8, so they should have a high weight for an 8. \n",
    "\n",
    "This can be represented as a function with a set of weight values for each possible category, for instance the probability of being the number 8 =>\n",
    "\n",
    "```python\n",
    "def pr_eight(image_vector, weights_vector): return (image_vector * weights_vector).sum()\n",
    "```\n",
    "\n",
    "Here, the image represented as a vector is basically \"all the rows stacked up end to end on a single long line\". With this kind of function, we just need some way to update the weights to make them better and better at distinguishing between digits: we want to find the specific values for the weights that cause the result of our function to be high for images that are actually 8s, and low for images that are not. Searching for the best weights is a way of searching for the best function that recognizes 8s.\n",
    "\n",
    "These are the steps that will turn our function into a machine learning classifier:\n",
    "\n",
    "1. _initialize_ the weights with random values\n",
    "2. for each image, use these weights to predict whether it appears to be a given digit\n",
    "3. based on these predictions, calculate how good the model is (its _loss_)\n",
    "4. calculate the _gradient_, which measures for each weight, how changing that weight would change the loss; the gradients will tell us how much we have to change each weight to make our model better\n",
    "5. _step_ (that is, _change_) all the weights based on the previous calculation\n",
    "6. go back to step 2 and repeat the process\n",
    "7. iterate until you decide to stop the training process (for instance, because the model is good enough or you don't want to wait any longer). Generally, you would want to stop the training process when the model stops improving or gets worse\n",
    "\n",
    "These relatively simple steps are the basis for nearly all deep learning models.\n",
    "\n",
    "When we know how a function changes, we know what we need to make its value smaller. This is what weights assignment is all about.\n",
    "\n",
    "One important thing to be aware of is that our function has lots of weights that we need to adjust, so when we calculate the derivative of these weights we won't get back one number, but rather lots of them: a _gradient_ for every weight. But there is nothing mathematically tricky here; you can calculate the derivative with respect to one weight, and treat all the other ones as constant, then repeat that for each other weight. This is how all of the gradients are calculated, for every weight.\n",
    "\n",
    "Thankfully, `PyTorch` can automatically compute the derivative of nearly any function!\n",
    "Adjusting the weights of our model can be then expressed as simply as:\n",
    "\n",
    "```python\t\n",
    "weights -= gradient * lr\n",
    "```\n",
    "\n",
    "This is what _stepping the weights_ actually means.\n",
    "\n",
    "We use a substraction here to allow for:\n",
    "\n",
    "- in case the slope is positive, the weight to be decreased\n",
    "- in case the slope is negative, the weight to be increased\n",
    "\n",
    "Remember: our ultimate goal is to minimize the loss.\n",
    "\n",
    "Let's summarize a few points here:\n",
    "\n",
    "- The weights of a model can be random (training a model from scratch) or they can come from a pre trained model (transfer learning).\n",
    "- If you train a model from scratch, the first outputs you'll get will probably be pretty bad, because your weights are random.\n",
    "- Also, in case of a pretrained model, the first outputs won't probably be what you want, because the model was trained for a different task. So, the model will need to _learn_ better weights.\n",
    "- To measure the performance of a model, you compare the outputs the model gives you with the targets (the true labels), using a loss function.\n",
    "- We want the output of the loss function to be as low as possible by improving the weights of the model.\n",
    "- For improving the model, you'd want to set aside a few data items that you don't train the model on, called the _validation set_. This is the set that will be used to measure performance.\n",
    "- Figuring out how to change the loss to make it better heavily relies on Calculus and gradients. This is made easy with tools like `PyTorch` ðŸ˜Ž.\n",
    "- At each pass of the iterative process that makes the weights better, we use the magnitude of each gradient to tell us how big a step is to take into a better direction. We multiply the gradients by a number called the _learning rate_ to decide each step size.\n",
    "- When the loss does not get any better, we say that the model has _converged_ and we stop the training.\n",
    "\n",
    "Alright, we are ready to apply this to the MNIST dataset now !\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining training and validation sets + parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6131, 28, 28])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create lists of tensors containing all digits (one for every digit)\n",
    "list_0s = [tensor(Image.open(digit)) for digit in mnist_training_0s]\n",
    "list_1s = [tensor(Image.open(digit)) for digit in mnist_training_1s]\n",
    "list_2s = [tensor(Image.open(digit)) for digit in mnist_training_2s]\n",
    "list_3s = [tensor(Image.open(digit)) for digit in mnist_training_3s]\n",
    "list_4s = [tensor(Image.open(digit)) for digit in mnist_training_4s]\n",
    "list_5s = [tensor(Image.open(digit)) for digit in mnist_training_5s]\n",
    "list_6s = [tensor(Image.open(digit)) for digit in mnist_training_6s]\n",
    "list_7s = [tensor(Image.open(digit)) for digit in mnist_training_7s]\n",
    "list_8s = [tensor(Image.open(digit)) for digit in mnist_training_8s]\n",
    "list_9s = [tensor(Image.open(digit)) for digit in mnist_training_9s]\n",
    "\n",
    "list_0s_test = [tensor(Image.open(digit)) for digit in mnist_testing_0s]\n",
    "list_1s_test = [tensor(Image.open(digit)) for digit in mnist_testing_1s]\n",
    "list_2s_test = [tensor(Image.open(digit)) for digit in mnist_testing_2s]\n",
    "list_3s_test = [tensor(Image.open(digit)) for digit in mnist_testing_3s]\n",
    "list_4s_test = [tensor(Image.open(digit)) for digit in mnist_testing_4s]\n",
    "list_5s_test = [tensor(Image.open(digit)) for digit in mnist_testing_5s]\n",
    "list_6s_test = [tensor(Image.open(digit)) for digit in mnist_testing_6s]\n",
    "list_7s_test = [tensor(Image.open(digit)) for digit in mnist_testing_7s]\n",
    "list_8s_test = [tensor(Image.open(digit)) for digit in mnist_testing_8s]\n",
    "list_9s_test = [tensor(Image.open(digit)) for digit in mnist_testing_9s]\n",
    "\n",
    "# let's stack our images and cast our stacked tensors as float tensors for further computation,\n",
    "# we also normalise our pixel values to be between 0 and 1 by dividing by 255, this will improve our model's accuracy\n",
    "stacked_0s = torch.stack(list_0s).float()/255\n",
    "stacked_1s = torch.stack(list_1s).float()/255\n",
    "stacked_2s = torch.stack(list_2s).float()/255\n",
    "stacked_3s = torch.stack(list_3s).float()/255\n",
    "stacked_4s = torch.stack(list_4s).float()/255\n",
    "stacked_5s = torch.stack(list_5s).float()/255\n",
    "stacked_6s = torch.stack(list_6s).float()/255\n",
    "stacked_7s = torch.stack(list_7s).float()/255\n",
    "stacked_8s = torch.stack(list_8s).float()/255\n",
    "stacked_9s = torch.stack(list_9s).float()/255\n",
    "# a _shape_ tells us the length of each axis of a tensor\n",
    "stacked_3s.shape # the output says \"we have 6131 images, each of size 28x28 pixels\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our digits classifier issue to solve, we already have independent variables (x-axis): these are the images themselves. This means we can put together our training dataset.\n",
    "\n",
    "Let's concatenate all of them into a single tensor.\n",
    "\n",
    "To do this, we'll need to go from a rank-3 tensor of images stacked together to a list of vectors (rank-2 tensor). This can be done using the `view` method of `PyTorch` tensors, as it changes the shape of a tensor without changing its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `torch.cat` concatenates tensors along the first dimension;\n",
    "# then `view` reshapes the concatenated tensor into a rank-2 tensor,\n",
    "# this new tensor has 28*28 columns and a number of rows equal to the number of images in the concatenated tensors;\n",
    "# a 28*28 image is flattened into a 784 pixels vector\n",
    "train_x = torch.cat([stacked_0s, stacked_1s, stacked_2s, stacked_3s, \n",
    "    stacked_4s, stacked_5s, stacked_6s, stacked_7s, \n",
    "    stacked_8s, stacked_9s]).view(-1, 28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's label each image\n",
    "train_y = tensor([1]*len(mnist_threes) + [0]*len(mnist_sevens)).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line of code above:\n",
    "\n",
    "- concatenates 2 tensors of length `len(threes)` and `len(sevens)` respectively\n",
    "- the first elements of this new tensor (`len(threes)`) are set to 0\n",
    "- the last elements of this new tensor (`len(sevens)`) are set to 1\n",
    "- calling `unsqueeze` on a tensor adds an extra dimension to it\n",
    "- we add one extra dimension to match the shape of the images tensor that will be fed to the model\n",
    "- the resulting tensor has a shape of `(n, 1)`, where `n` is the number of images in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12396, 784]), torch.Size([12396, 1]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape,train_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `Dataset` in `PyTorch` is an abstract class that represents a collection of data samples.\n",
    "\n",
    "When indexed, a `Dataset` is required to return a tuple of `(x,y)`, where `x` is the input data and `y` is the label.\n",
    "\n",
    "Let's create our `Dataset` of images and labels using the `zip` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([784]), tensor([1]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set = list(zip(train_x,train_y))\n",
    "\n",
    "# taking a look at the first element of the dataset,\n",
    "# here, `y` represents the label of the image\n",
    "x,y = training_set[0] \n",
    "x.shape,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, let's put together our validation set\n",
    "valid_x = torch.cat([mnist_validation_threes, mnist_validation_sevens]).view(-1, 28*28)\n",
    "valid_y = tensor([1]*len(mnist_validation_threes) + [0]*len(mnist_validation_sevens)).unsqueeze(1)\n",
    "validation_set = list(zip(valid_x,valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9269],\n",
       "        [ 1.4873],\n",
       "        [ 0.9007],\n",
       "        [-2.1055],\n",
       "        [ 0.6784]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's initialize our weights for every pixel with random values\n",
    "\n",
    "# this function returns a tensor of size `size` filled with random values from a normal distribution with a standard deviation of `std`\n",
    "def init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_()\n",
    "\n",
    "weights = init_params((28*28,1))\n",
    "# print a sample of our weights\n",
    "weights[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the goal is to multiply each pixel of the images by a weight value, we should take into account the fact that some pixels will have a value of `0`.\n",
    "\n",
    "This means that the function `weights * pixels` won't be flexible enough for our purpose since it can sometimes result in a value of `0` even if the weight is not `0`. That's not we want.\n",
    "\n",
    "Now, since the equation of a straight line is `y = wx + b`, we can add a constant `b` to our function to make it more flexible. This is called a _bias_.\n",
    "\n",
    "Together, the _weights_ and the _bias_ are called the _parameters_ of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3472], requires_grad=True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias = init_params(1)\n",
    "bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1051,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8.3031], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 1051,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we are now able to calculate a prediction for an image,\n",
    "# this is a simple implementation of a linear regression model;\n",
    "# `weights.T` means \"taking the transpose of the weights tensor\",\n",
    "# this is done because we want to multiply the weights by the pixels of the image,\n",
    "# so we turn the `weights` matrix rows into columns and vice versa\n",
    "(train_x[0]*weights.T).sum() + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we could use a Python for loop to calculate the prediction for each image, that would be very slow. Because Python loops don't run on the GPU, and because Python is a slow language for loops in general, we need to represent as much of the computation in a model as possible using higher-level functions.\n",
    "\n",
    "In this case, there's an extremely convenient mathematical operation that calculates `w*x` for every row of a matrix: matrix multiplication.\n",
    "\n",
    "In Python, matrix multiplication is represented with the `@` operator. Let's try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1052,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([784, 1])"
      ]
     },
     "execution_count": 1052,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1053,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12396, 784])"
      ]
     },
     "execution_count": 1053,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -6.2330],\n",
       "        [-10.6388],\n",
       "        [-20.8865],\n",
       "        ...,\n",
       "        [-15.9176],\n",
       "        [ -1.6866],\n",
       "        [-11.3568]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def multiply_data_by_params_model(independent_variables): return independent_variables@weights + bias\n",
    "preds = multiply_data_by_params_model(train_x)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equation `batch @ weights + bias` is a fundamental equation of any neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## checking our model's accuracy\n",
    "\n",
    "Let's check our accuracy. To decide if an output represents a 3 or a 7, we can just check whether it's greater than 0.0, since a 3 is a one and a 7 is a 0, so our accuracy for each item can be calculated (using broadcasting and no loops!) with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False],\n",
       "        [False],\n",
       "        [False],\n",
       "        ...,\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "corrects = (preds > 0.0).float() == train_y\n",
    "corrects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5379961133003235"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# our accuracy would be ...\n",
    "corrects.float().mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing a loss function\n",
    "\n",
    "As we've seen, we need gradients in order to improve our model using SGD, and in order to calculate gradients we need some loss function that represents how good our model is. That is because the gradients are a measure of how the loss function changes with small tweaks to the weights. So, we need to choose a loss function.\n",
    "\n",
    "A very small change of a weight won't influence the overall accuracy of the model, so we can exclude accuracy as our loss function.\n",
    "\n",
    "Instead, we need a loss function which, when our weights result in slightly better predictions, gives us a slightly better loss. So what does a \"slightly better prediction\" look like, exactly? Well, in this case, it means that if the correct answer is a 3 the score is a little higher, or if the correct answer is a 7 the score is a little lower.\n",
    "\n",
    "The loss function receives not the images themselves, but the predictions from the model. Let's make one argument, `predictions`, of values between 0 and 1, where each value is the prediction that an image is a 3. It is a vector (i.e., a rank-1 tensor), indexed over the images.\n",
    "\n",
    "The purpose of the loss function is to measure the difference between predicted values and the true values â€” that is, the targets (aka labels). Let's make another parameter, `targets`, with values of 0 or 1 which tells whether an image actually is a 3 or not. It is also a vector (i.e., another rank-1 tensor), indexed over the images.\n",
    "\n",
    "Let's try this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure how distant each prediction is from 1 if it should be 1, \n",
    "# and how distant it is from 0 if it should be 0, \n",
    "# and then take the mean of all those distances\n",
    "\n",
    "def mnist_loss(predictions, targets):\n",
    "    # The 'torch.where' function takes three arguments: condition, x, and y.\n",
    "    # It returns a new tensor where each element comes from either 'x' or 'y', depending on the corresponding element in 'condition'.\n",
    "    return torch.where(\n",
    "        targets == 1,  # Condition: Check where the targets are 1\n",
    "        1 - predictions,  # x: If target is 1, loss is (1 - prediction)\n",
    "        predictions  # y: If target is 0, loss is (prediction)\n",
    "    ).mean()  # Calculate the mean of the resulting tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.where(condition, x, y)` is a PyTorch function that takes a condition and two tensors x and y. For each element in the condition tensor, it will choose the corresponding element from x if the condition is True, and from y if the condition is False.\n",
    "\n",
    "The function above will return a lower number when:\n",
    "\n",
    "- predictions are more accurate\n",
    "- accurate predictions are more confident\n",
    "- inaccurate predictions are less confident\n",
    "\n",
    "One problem with `mnist_loss` as currently defined is that it assumes that predictions are always between 0 and 1. We need to ensure, then, that this is actually the case! As it happens, there is a function that does exactly that: the _sigmoid_ function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's update the `mnist_loss` function to use the sigmoid function\n",
    "def mnist_loss(predictions, targets):\n",
    "    predictions = predictions.sigmoid()\n",
    "    return torch.where(targets==1, 1-predictions, predictions).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting our linear modal together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256, 784]), torch.Size([256, 1]))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize our parameters\n",
    "weights = init_params((28*28,1))\n",
    "bias = init_params(1)\n",
    "\n",
    "# creating a data loader that will return batches of a given size shuffled for every epoch\n",
    "training_data_loader = DataLoader(training_set, batch_size=256)\n",
    "x,y = first(training_data_loader)\n",
    "x.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a data loader for the validation set\n",
    "validation_data_loader = DataLoader(validation_set, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's define a few functions that will help us calculate the gradients and update the weights and biases\n",
    "\n",
    "# let's also write a function that calculates accuracy for each batch\n",
    "def batch_accuracy(independent_variables, dependent_variables):\n",
    "    preds = independent_variables.sigmoid()\n",
    "    correct = (preds > 0.5) == dependent_variables\n",
    "    return correct.float().mean()\n",
    "\n",
    "# let's put this logic into a function for:\n",
    "#  - making predictions\n",
    "#  - calculating the loss\n",
    "#  - calculating the gradients\n",
    "def calc_grad(independent_variables, dependent_variables, model):\n",
    "    preds = model(independent_variables)\n",
    "    loss = mnist_loss(preds, dependent_variables)\n",
    "    # The `backward` function actually adds the gradients of `loss` to any gradients that are currently stored.\n",
    "    loss.backward()\n",
    "\n",
    "# we need to update the weights and biases based on the gradient and learning rate\n",
    "def train_epoch(model, lr, params):\n",
    "    for x, y in training_data_loader:\n",
    "        calc_grad(x, y, model)\n",
    "        for p in params:\n",
    "            p.data -= p.grad*lr\n",
    "            # we have to tell `PyTorch` to not store the gradients for this step,\n",
    "            # otherwise they would accumulate and things could get very confusing\n",
    "            p.grad.zero_()\n",
    "\n",
    "# let's write a validation function,\n",
    "# this function `validate_epoch` takes a PyTorch model as input and evaluates its accuracy on a validation dataset.\n",
    "# It does so by iterating over batches of data in the validation dataloader, \n",
    "# passing each batch through the model to get the predicted outputs, \n",
    "# and comparing those predictions to the true labels (`dependent_variables`) to calculate the accuracy of each batch. \n",
    "# The function then returns the average accuracy across all batches, rounded to 4 decimal places.\n",
    "def validate_epoch(model):\n",
    "    accs = [batch_accuracy(model(independent_variables), dependent_variables) for independent_variables, dependent_variables in validation_data_loader]\n",
    "    return round(torch.stack(accs).mean().item(), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6546"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's run a first iteration of our model\n",
    "lr = 1.\n",
    "params = weights, bias\n",
    "train_epoch(multiply_data_by_params_model, lr, params)\n",
    "validate_epoch(multiply_data_by_params_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8158 0.8915 0.9291 0.9413 0.9486 0.953 0.956 0.9574 0.9589 0.9599 "
     ]
    }
   ],
   "source": [
    "# then let's do a few more rounds\n",
    "for i in range(10):\n",
    "    train_epoch(multiply_data_by_params_model, lr, params)\n",
    "    print(validate_epoch(multiply_data_by_params_model), end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... this is pretty promising !\n",
    "\n",
    "Now let's create an object that will handle the SGD steps for us. In `PyTorch`, it's called an _optimizer_.\n",
    "\n",
    "There is a module in `PyTorch` called `nn.Linear`: it does the same things as `init_params` and `multiply_data_by_params_model` together.\n",
    "\n",
    "It will contain both the weights and the biases of our model in a single class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 784]), torch.Size([1]))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model = nn.Linear(28*28,1)\n",
    "\n",
    "weights, biases = linear_model.parameters()\n",
    "weights.shape, biases.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use that to create our basic optimizer\n",
    "class BasicOptim:\n",
    "    def __init__(self,params,lr): self.params,self.lr = list(params),lr\n",
    "\n",
    "    def step(self, *args, **kwargs):\n",
    "        for p in self.params: p.data -= p.grad.data * self.lr\n",
    "\n",
    "    def zero_grad(self, *args, **kwargs):\n",
    "        for p in self.params: p.grad = None\n",
    "\n",
    "# let's make an instance of it using our model's parameters\n",
    "opt = BasicOptim(linear_model.parameters(), lr)   \n",
    "\n",
    "# this simplifies our training loop to:\n",
    "def train_epoch(model):\n",
    "    for x, y in training_data_loader:\n",
    "        calc_grad(x, y, model)\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "# we can now write a function that trains our model:\n",
    "def train_model(model, epochs):\n",
    "    for i in range(epochs):\n",
    "        train_epoch(model)\n",
    "        print(validate_epoch(model), end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4932 0.8262 0.8408 0.9126 0.9341 0.9468 0.9555 0.9629 0.9653 0.9668 0.9692 0.9717 0.9731 0.9751 0.9761 0.9765 0.9775 0.9785 0.9785 0.9785 "
     ]
    }
   ],
   "source": [
    "# let's try it out\n",
    "train_model(linear_model, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we have just did with `BasicOptim` is implemented in `fastai` with the `SGD` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4932 0.749 0.8579 0.917 0.9365 0.9521 0.957 0.9629 0.9658 0.9687 0.9707 0.9721 0.9731 0.9746 0.9761 0.9765 0.9775 0.978 0.978 0.978 "
     ]
    }
   ],
   "source": [
    "linear_model = nn.Linear(28*28,1)\n",
    "opt = SGD(linear_model.parameters(), lr)\n",
    "train_model(linear_model, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, the implementation of our `train_model` function in `fastai` is called `Learner.fit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = DataLoaders(training_data_loader, validation_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are about to create a `Learner` without using an application such as `vision_learner`. For this, we can pass in all the elements that we've created: \n",
    "\n",
    "- any metrics we want to use (accuracy in this case)\n",
    "- the data loaders\n",
    "- the loss function\n",
    "- the model\n",
    "- the optimization function, which will be passed the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>batch_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.636516</td>\n",
       "      <td>0.503599</td>\n",
       "      <td>0.495584</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.567307</td>\n",
       "      <td>0.194196</td>\n",
       "      <td>0.835623</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.207229</td>\n",
       "      <td>0.181414</td>\n",
       "      <td>0.835132</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.089765</td>\n",
       "      <td>0.106924</td>\n",
       "      <td>0.910206</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.046562</td>\n",
       "      <td>0.078032</td>\n",
       "      <td>0.932777</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.029760</td>\n",
       "      <td>0.062546</td>\n",
       "      <td>0.947007</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.022909</td>\n",
       "      <td>0.052922</td>\n",
       "      <td>0.955839</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.019906</td>\n",
       "      <td>0.046520</td>\n",
       "      <td>0.961236</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.018408</td>\n",
       "      <td>0.042010</td>\n",
       "      <td>0.965162</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.017523</td>\n",
       "      <td>0.038679</td>\n",
       "      <td>0.967125</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner = Learner(dls, nn.Linear(28*28,1), opt_func=SGD, loss_func=mnist_loss, metrics=batch_accuracy)\n",
    "\n",
    "# now we can call `fit` to train our model across multiple epochs\n",
    "learner.fit(10, lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing our own neural network\n",
    "\n",
    "Let's try to write a very basic neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an illustration of the universal approximation theorem\n",
    "def simple_net(independent_variables): \n",
    "    res = independent_variables@w1 + b1\n",
    "    # here we have our relu activation function\n",
    "    res = res.max(tensor(0.0))\n",
    "    res = res@w2 + b2\n",
    "    return res\n",
    "\n",
    "# let's use this\n",
    "# here `w1` is the weights matrix connecting the input layer to a layer of 30 neurons (the hidden layer)\n",
    "w1 = init_params((28*28,30))\n",
    "b1 = init_params(30)\n",
    "# here `w2` is the weights matrix connecting the hidden layer to the output layer, which has a single neuron (since we're just trying to predict one number, 3 or 7)\n",
    "w2 = init_params((30,1))\n",
    "b2 = init_params(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, the first hidden layer can construct 30 different features, each representing some different mix of pixels. Changing the value `30` will result in a more or less complex model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>batch_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.334446</td>\n",
       "      <td>0.408529</td>\n",
       "      <td>0.505397</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.154280</td>\n",
       "      <td>0.237300</td>\n",
       "      <td>0.792444</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.084378</td>\n",
       "      <td>0.118382</td>\n",
       "      <td>0.910697</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.054719</td>\n",
       "      <td>0.079430</td>\n",
       "      <td>0.939647</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.041056</td>\n",
       "      <td>0.061905</td>\n",
       "      <td>0.956330</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.034140</td>\n",
       "      <td>0.052130</td>\n",
       "      <td>0.963199</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.030212</td>\n",
       "      <td>0.045985</td>\n",
       "      <td>0.964181</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.027690</td>\n",
       "      <td>0.041786</td>\n",
       "      <td>0.966143</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.025886</td>\n",
       "      <td>0.038731</td>\n",
       "      <td>0.968106</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.024494</td>\n",
       "      <td>0.036397</td>\n",
       "      <td>0.969087</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.023369</td>\n",
       "      <td>0.034541</td>\n",
       "      <td>0.972522</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.022430</td>\n",
       "      <td>0.033021</td>\n",
       "      <td>0.972522</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.021631</td>\n",
       "      <td>0.031744</td>\n",
       "      <td>0.973013</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.020940</td>\n",
       "      <td>0.030651</td>\n",
       "      <td>0.973994</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.020336</td>\n",
       "      <td>0.029700</td>\n",
       "      <td>0.974975</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.019802</td>\n",
       "      <td>0.028860</td>\n",
       "      <td>0.976448</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.019324</td>\n",
       "      <td>0.028115</td>\n",
       "      <td>0.976938</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.018894</td>\n",
       "      <td>0.027446</td>\n",
       "      <td>0.977920</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.018504</td>\n",
       "      <td>0.026845</td>\n",
       "      <td>0.978410</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.018149</td>\n",
       "      <td>0.026299</td>\n",
       "      <td>0.978901</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.017822</td>\n",
       "      <td>0.025802</td>\n",
       "      <td>0.978901</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.017520</td>\n",
       "      <td>0.025347</td>\n",
       "      <td>0.978901</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.017240</td>\n",
       "      <td>0.024929</td>\n",
       "      <td>0.979882</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.016979</td>\n",
       "      <td>0.024544</td>\n",
       "      <td>0.979882</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.016734</td>\n",
       "      <td>0.024187</td>\n",
       "      <td>0.980373</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.016504</td>\n",
       "      <td>0.023857</td>\n",
       "      <td>0.979882</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.016288</td>\n",
       "      <td>0.023551</td>\n",
       "      <td>0.980373</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.016083</td>\n",
       "      <td>0.023266</td>\n",
       "      <td>0.980373</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.015890</td>\n",
       "      <td>0.022999</td>\n",
       "      <td>0.980864</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.015706</td>\n",
       "      <td>0.022751</td>\n",
       "      <td>0.981845</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.015531</td>\n",
       "      <td>0.022518</td>\n",
       "      <td>0.981845</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.015365</td>\n",
       "      <td>0.022300</td>\n",
       "      <td>0.981845</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.015206</td>\n",
       "      <td>0.022095</td>\n",
       "      <td>0.982336</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.015054</td>\n",
       "      <td>0.021903</td>\n",
       "      <td>0.982336</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.014908</td>\n",
       "      <td>0.021721</td>\n",
       "      <td>0.982336</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.014769</td>\n",
       "      <td>0.021549</td>\n",
       "      <td>0.982336</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.014634</td>\n",
       "      <td>0.021387</td>\n",
       "      <td>0.982336</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.014505</td>\n",
       "      <td>0.021233</td>\n",
       "      <td>0.982826</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.014381</td>\n",
       "      <td>0.021087</td>\n",
       "      <td>0.982336</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.014261</td>\n",
       "      <td>0.020948</td>\n",
       "      <td>0.982336</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# `PyTorch` version of our simple neural network\n",
    "simple_net = nn.Sequential(\n",
    "    nn.Linear(28*28,30),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(30,1)\n",
    ")\n",
    "\n",
    "learn = Learner(dls, simple_net, opt_func=SGD,\n",
    "    loss_func=mnist_loss, metrics=batch_accuracy)\n",
    "\n",
    "learn.fit(40, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f6541587bb0>]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting the accuracy training, the x-axis is the epoch number and the y-axis is the accuracy\n",
    "plt.plot(L(learn.recorder.values).itemgot(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.065239</td>\n",
       "      <td>0.017955</td>\n",
       "      <td>0.995584</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's try more layers with a pre trained model\n",
    "dls = ImageDataLoaders.from_folder(path_to_mnist)\n",
    "learn = vision_learner(dls, resnet18, pretrained=False,\n",
    "    loss_func=F.cross_entropy, metrics=accuracy)\n",
    "learn.fit_one_cycle(1, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See what we can do with fewer epochs and more layers ? That's impressive !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
