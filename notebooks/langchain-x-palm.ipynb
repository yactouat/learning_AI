{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain X PaLM\n",
    "\n",
    "This notebook contains the code for the LangChain + PaLM blog articles available on https://yactouat.com/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting things up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from google.cloud import aiplatform\n",
    "import langchain\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chat_models import ChatVertexAI\n",
    "from langchain.embeddings import VertexAIEmbeddings\n",
    "from langchain.llms import VertexAI\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "# supporting type hints\n",
    "from typing import List\n",
    "import vertexai\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertex AI SDK version: 1.37.0\n",
      "LangChain version: 0.0.346\n"
     ]
    }
   ],
   "source": [
    "# `GCP_LOCATION` is for instance `europe-west1`\n",
    "vertexai.init(project=os.getenv('GCP_PROJECT_ID'), location=os.getenv('GCP_LOCATION')) \n",
    "\n",
    "print(f\"Vertex AI SDK version: {aiplatform.__version__}\")\n",
    "\n",
    "print(f\"LangChain version: {langchain.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function to rate limit API calls\n",
    "def rate_limit(max_per_minute):\n",
    "    period = 60 / max_per_minute\n",
    "    print(\"Waiting\")\n",
    "    while True:\n",
    "        before = time.time()\n",
    "        # the `yield` keyword here controls back to the caller;\n",
    "        # e.g. it allows the caller to perform the action that is rate limited\n",
    "        yield\n",
    "        # at this points, control returns to this function\n",
    "        after = time.time()\n",
    "        elapsed = after - before\n",
    "        sleep_time = max(0, period - elapsed)\n",
    "        if sleep_time > 0:\n",
    "            print(\".\", end=\"\")\n",
    "            time.sleep(sleep_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomVertexAIEmbeddings():\n",
    "    def __init__(self, requests_per_minute: int, num_instances_per_batch: int):\n",
    "        self.requests_per_minute = requests_per_minute\n",
    "        self.num_instances_per_batch = num_instances_per_batch\n",
    "\n",
    "    # takes a list of strings and returns a list of embeddings\n",
    "    def embed_documents(self, texts: List[str]):\n",
    "        limiter = rate_limit(self.requests_per_minute)\n",
    "        results = []\n",
    "        remaining = list(texts)\n",
    "\n",
    "        while remaining:\n",
    "            # working in batches because the API accepts maximum 5\n",
    "            # documents per request to get embeddings\n",
    "            batch, remaining = (\n",
    "                # batch contains the first `num_instances_per_batch` documents\n",
    "                remaining[: self.num_instances_per_batch],\n",
    "                # docs contains the remaining documents\n",
    "                remaining[self.num_instances_per_batch :],\n",
    "            )\n",
    "            self.vertex_ai_embeddings = VertexAIEmbeddings()\n",
    "            chunk = self.vertex_ai_embeddings.client.get_embeddings(batch)\n",
    "            results.extend(chunk)\n",
    "            next(limiter)\n",
    "\n",
    "        return [r.values for r in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a foundational LLM instance\n",
    "# LLM model\n",
    "llm = VertexAI(\n",
    "    model_name=\"text-bison-32k@002\",\n",
    "    max_output_tokens=8192,\n",
    "    # setting the temperature that low will make the mode more deterministic\n",
    "    temperature=0.5,\n",
    "    # (nucleus sampling): the model will only consider a cumulative probability threshold above 0.8 to consider words to use\n",
    "    top_p=0.8,\n",
    "    # the model will only consider the top k most likely words when generating text\n",
    "    top_k=40,\n",
    "    verbose=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set embeddings\n",
    "# `QPM` stands for \"queries per minute\"\n",
    "EMBEDDING_QPM = 100\n",
    "EMBEDDING_NUM_BATCH = 5\n",
    "embeddings = CustomVertexAIEmbeddings(\n",
    "    requests_per_minute=EMBEDDING_QPM,\n",
    "    num_instances_per_batch=EMBEDDING_NUM_BATCH,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic interactions with an LLM\n",
    "\n",
    "Text is the natural way of interacting with LLMs, so let's just do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The future of AI multimodality holds immense potential for revolutionizing various industries and transforming how we interact with technology. Here are some key trends and developments to watch out for:\\n\\n1. **Enhanced Human-Machine Interaction**: AI multimodality will continue to enhance human-machine interaction by enabling seamless communication and collaboration between humans and machines. This will involve advancements in natural language processing, speech recognition, gesture recognition, and other multimodal interfaces.\\n\\n2. **Immersive Experiences**: AI multimodality will play a crucial role in creating immersive and engaging experiences across various domains. This includes virtual reality (VR), augmented reality (AR), and mixed reality (MR), where users can interact with digital content in a more natural and intuitive way.\\n\\n3. **Cross-Modal Understanding**: AI models will become increasingly adept at understanding and interpreting information from different modalities, such as text, images, audio, and video. This cross-modal understanding will enable more comprehensive and accurate analysis and decision-making.\\n\\n4. **Healthcare and Medical Applications**: AI multimodality will find widespread applications in healthcare, such as medical diagnosis, treatment planning, and personalized patient care. By analyzing multimodal data, including medical images, electronic health records, and patient-reported outcomes, AI systems can provide more accurate and timely insights.\\n\\n5. **Autonomous Systems**: AI multimodality will be essential for the development of autonomous systems, such as self-driving cars and robots. These systems require the ability to perceive and interpret the environment through multiple sensors, including cameras, LiDAR, and radar.\\n\\n6. **Emotion Recognition and Analysis**: AI multimodality will enable machines to better understand and respond to human emotions. This will have applications in customer service, mental health, and social robotics, where machines can adapt their behavior based on emotional cues.\\n\\n7. **Art and Creativity**: AI multimodality will empower artists and creative professionals to explore new forms of artistic expression. By combining different modalities, such as text, images, music, and movement, artists can create immersive and interactive experiences that transcend traditional boundaries.\\n\\n8. **Ethical Considerations**: As AI multimodality becomes more sophisticated, ethical considerations will become increasingly important. This includes issues related to privacy, data security, bias, and the responsible use of AI systems.\\n\\nOverall, the future of AI multimodality holds tremendous promise for advancing technology and transforming various aspects of our lives. By harnessing the power of multiple modalities, AI systems will become more intelligent, intuitive, and capable of delivering personalized and immersive experiences.'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interaction = \"what is the future of AI multi modality?\"\n",
    "response = llm(interaction)\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Multimodal AI systems will become increasingly sophisticated, enabling them to process and understand a wider range of data types, including text, images, audio, and video. This will allow them to perform a broader range of tasks, such as generating creative content, providing personalized recommendations, and making complex decisions.\n"
     ]
    }
   ],
   "source": [
    "chat = ChatVertexAI(model_name=\"chat-bison-32k@002\")\n",
    "\n",
    "initial_human_message = HumanMessage(\n",
    "    content=\"What is the future of AI multi modality?\",\n",
    ")\n",
    "\n",
    "chat_interaction = [\n",
    "    SystemMessage(\n",
    "        content=\"You are an AI assistant that is specialized in software enginering and in machine learning. Your answers are two short sentences long maximum.\",\n",
    "    ),\n",
    "    initial_human_message\n",
    "]\n",
    "\n",
    "response = chat(chat_interaction)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Multimodal AI systems will be able to process and understand a wider range of data types, including text, images, audio, video, and even haptic data. This will allow them to interact with the world in a more natural and intuitive way.\n"
     ]
    }
   ],
   "source": [
    "response_with_history = chat([\n",
    "    SystemMessage(\n",
    "        content=\"You are an AI assistant that is specialized in software enginering and in machine learning. Your answers are two short sentences long maximum.\",\n",
    "    ),\n",
    "    initial_human_message,\n",
    "    AIMessage(\n",
    "        content=response.content,\n",
    "    ),\n",
    "    HumanMessage(content=\"what wider range of data types?\")\n",
    "])\n",
    "\n",
    "print(response_with_history.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[-0.033491309732198715,\n",
       "  -0.024705076590180397,\n",
       "  0.04814573749899864,\n",
       "  -0.0027620443142950535,\n",
       "  -0.006553742568939924],\n",
       " [-0.011842082254588604,\n",
       "  0.0037548220716416836,\n",
       "  -0.010157657787203789,\n",
       "  -0.009990965016186237,\n",
       "  0.03311667591333389],\n",
       " [0.018089881166815758,\n",
       "  -0.012067587114870548,\n",
       "  0.02881603129208088,\n",
       "  0.05261128023266792,\n",
       "  -0.003405238501727581]]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here's how you can get an embedding for a sentence\n",
    "text = \"I love Shakespeare\"\n",
    "text2 = \"Life without theaters would be dull\"\n",
    "# using a completely unrelated sentence\n",
    "text3 = \"Hey honey where did you put the keys?\"\n",
    "\n",
    "# sending the text to create embeddings for to Vertex AI\n",
    "text_embeddings = embeddings.embed_documents([text, text2, text3])\n",
    "# showing only the first 5 values of each embedding\n",
    "text_embeddings_short = [e[:5] for e in text_embeddings]\n",
    "text_embeddings_short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9161093776749432, 1.0209345760221844)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the cosine similarity between the first two embeddings\n",
    "euclidian_distance_1_2 = np.linalg.norm(np.array(text_embeddings[0]) - np.array(text_embeddings[1]))\n",
    "# now between the first and the third\n",
    "euclidian_distance_1_3 = np.linalg.norm(np.array(text_embeddings[0]) - np.array(text_embeddings[2]))\n",
    "\n",
    "euclidian_distance_1_2, euclidian_distance_1_3\n",
    "# we can see that the first two embeddings are closer to each other than the first and the third\n",
    "# ! watch out, this is just an example as the magnitude of the embeddings is not normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5803716847710505, 0.4788461710635313)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now let's do the same thing using cosine similarity\n",
    "cosine_similarity_1_2 = np.dot(text_embeddings[0], text_embeddings[1]) / (\n",
    "    np.linalg.norm(text_embeddings[0]) * np.linalg.norm(text_embeddings[1])\n",
    ")\n",
    "cosine_similarity_1_3 = np.dot(text_embeddings[0], text_embeddings[2]) / (\n",
    "    np.linalg.norm(text_embeddings[0]) * np.linalg.norm(text_embeddings[2])\n",
    ")\n",
    "\n",
    "cosine_similarity_1_2, cosine_similarity_1_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The steam engine, a pivotal invention of the Industrial Revolution, revolutionized industries and transportation. While its significance cannot be overstated, its relevance in today's world is limited due to the advent of more efficient and environmentally friendly technologies.\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "What do you think of this technology {tech}? Is it still relevant for use today?\n",
    "\n",
    "Your answers are two short sentences long maximum.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"tech\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "final_prompt = prompt.format(tech=\"steam engine\")\n",
    "response = llm(final_prompt)\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
