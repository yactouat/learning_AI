{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain X PaLM\n",
    "\n",
    "This notebook contains the code for the LangChain + PaLM tutorial available on https://yactouat.com/langchain-palm-getting-started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting things up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from google.cloud import aiplatform\n",
    "import langchain\n",
    "from langchain.embeddings import VertexAIEmbeddings\n",
    "from langchain.llms import VertexAI\n",
    "import os\n",
    "import time\n",
    "# supporting type hints\n",
    "from typing import List\n",
    "import vertexai\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertex AI SDK version: 1.37.0\n",
      "LangChain version: 0.0.346\n"
     ]
    }
   ],
   "source": [
    "# `GCP_LOCATION` is for instance `europe-west1`\n",
    "vertexai.init(project=os.getenv('GCP_PROJECT_ID'), location=os.getenv('GCP_LOCATION')) \n",
    "\n",
    "print(f\"Vertex AI SDK version: {aiplatform.__version__}\")\n",
    "\n",
    "print(f\"LangChain version: {langchain.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function to rate limit API calls\n",
    "def rate_limit(max_per_minute):\n",
    "    period = 60 / max_per_minute\n",
    "    print(\"Waiting\")\n",
    "    while True:\n",
    "        before = time.time()\n",
    "        # the `yield` keyword here controls back to the caller;\n",
    "        # e.g. it allows the caller to perform the action that is rate limited\n",
    "        yield\n",
    "        # at this points, control returns to this function\n",
    "        after = time.time()\n",
    "        elapsed = after - before\n",
    "        sleep_time = max(0, period - elapsed)\n",
    "        if sleep_time > 0:\n",
    "            print(\".\", end=\"\")\n",
    "            time.sleep(sleep_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomVertexAIEmbeddings():\n",
    "    def __init__(self, requests_per_minute: int, num_instances_per_batch: int):\n",
    "        self.requests_per_minute = requests_per_minute\n",
    "        self.num_instances_per_batch = num_instances_per_batch\n",
    "\n",
    "    # takes a list of strings and returns a list of embeddings\n",
    "    def embed_documents(self, texts: List[str]):\n",
    "        limiter = rate_limit(self.requests_per_minute)\n",
    "        results = []\n",
    "        remaining = list(texts)\n",
    "\n",
    "        while remaining:\n",
    "            # working in batches because the API accepts maximum 5\n",
    "            # documents per request to get embeddings\n",
    "            batch, remaining = (\n",
    "                # batch contains the first `num_instances_per_batch` documents\n",
    "                remaining[: self.num_instances_per_batch],\n",
    "                # docs contains the remaining documents\n",
    "                remaining[self.num_instances_per_batch :],\n",
    "            )\n",
    "            self.vertex_ai_embeddings = VertexAIEmbeddings()\n",
    "            chunk = self.vertex_ai_embeddings.client.get_embeddings(batch)\n",
    "            results.extend(chunk)\n",
    "            next(limiter)\n",
    "\n",
    "        return [r.values for r in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a foundational LLM instance\n",
    "# LLM model\n",
    "llm = VertexAI(\n",
    "    model_name=\"text-bison@002\",\n",
    "    max_output_tokens=256,\n",
    "    # setting the temperature that low will make the mode more deterministic\n",
    "    temperature=0.1,\n",
    "    # (nucleus sampling): the model will only consider a cumulative probability threshold above 0.8 to consider words to use\n",
    "    top_p=0.8,\n",
    "    # the model will only consider the top k most likely words when generating text\n",
    "    top_k=40,\n",
    "    verbose=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set embeddings\n",
    "# `QPM` stands for \"queries per minute\"\n",
    "EMBEDDING_QPM = 100\n",
    "EMBEDDING_NUM_BATCH = 5\n",
    "embeddings = CustomVertexAIEmbeddings(\n",
    "    requests_per_minute=EMBEDDING_QPM,\n",
    "    num_instances_per_batch=EMBEDDING_NUM_BATCH,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic interactions with an LLM\n",
    "\n",
    "Text is the natural way of interacting with LLMs, so let's just do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" The future of AI multimodality holds immense potential for transforming various industries and enhancing human experiences. Here are some key aspects to consider regarding the future of AI multimodality:\\n\\n**1. Enhanced Human-Computer Interaction:**\\nAI multimodality will enable more natural and intuitive interactions between humans and machines. By combining different modalities such as speech, gestures, facial expressions, and touch, AI systems can better understand user intent and provide personalized responses. This will revolutionize user interfaces, making them more user-friendly and efficient.\\n\\n**2. Improved Decision-Making:**\\nAI multimodality can significantly enhance decision-making processes by integrating data from multiple sources and modalities. By analyzing diverse information, AI systems can provide more accurate and comprehensive insights, leading to better decision-making outcomes in various fields such as healthcare, finance, and manufacturing.\\n\\n**3. Personalized Experiences:**\\nAI multimodality will enable the creation of highly personalized experiences tailored to individual users. By combining data from different modalities, AI systems can gain a deeper understanding of user preferences, behaviors, and contexts. This will allow for the delivery of personalized recommendations, content, and services that cater to each user's unique needs and interests.\\n\\n**4. Advanced Robotics and Autonomous Systems:**\\nAI\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interaction = \"what is the future of AI multi modality?\"\n",
    "response = llm(interaction)\n",
    "\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
