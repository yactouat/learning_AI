{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain X PaLM\n",
    "\n",
    "This notebook contains the code for the LangChain + PaLM tutorial available on https://yactouat.com/langchain-palm-getting-started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting things up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from google.cloud import aiplatform\n",
    "import langchain\n",
    "from langchain.chat_models import ChatVertexAI\n",
    "from langchain.embeddings import VertexAIEmbeddings\n",
    "from langchain.llms import VertexAI\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "import os\n",
    "import time\n",
    "# supporting type hints\n",
    "from typing import List\n",
    "import vertexai\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertex AI SDK version: 1.37.0\n",
      "LangChain version: 0.0.346\n"
     ]
    }
   ],
   "source": [
    "# `GCP_LOCATION` is for instance `europe-west1`\n",
    "vertexai.init(project=os.getenv('GCP_PROJECT_ID'), location=os.getenv('GCP_LOCATION')) \n",
    "\n",
    "print(f\"Vertex AI SDK version: {aiplatform.__version__}\")\n",
    "\n",
    "print(f\"LangChain version: {langchain.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function to rate limit API calls\n",
    "def rate_limit(max_per_minute):\n",
    "    period = 60 / max_per_minute\n",
    "    print(\"Waiting\")\n",
    "    while True:\n",
    "        before = time.time()\n",
    "        # the `yield` keyword here controls back to the caller;\n",
    "        # e.g. it allows the caller to perform the action that is rate limited\n",
    "        yield\n",
    "        # at this points, control returns to this function\n",
    "        after = time.time()\n",
    "        elapsed = after - before\n",
    "        sleep_time = max(0, period - elapsed)\n",
    "        if sleep_time > 0:\n",
    "            print(\".\", end=\"\")\n",
    "            time.sleep(sleep_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomVertexAIEmbeddings():\n",
    "    def __init__(self, requests_per_minute: int, num_instances_per_batch: int):\n",
    "        self.requests_per_minute = requests_per_minute\n",
    "        self.num_instances_per_batch = num_instances_per_batch\n",
    "\n",
    "    # takes a list of strings and returns a list of embeddings\n",
    "    def embed_documents(self, texts: List[str]):\n",
    "        limiter = rate_limit(self.requests_per_minute)\n",
    "        results = []\n",
    "        remaining = list(texts)\n",
    "\n",
    "        while remaining:\n",
    "            # working in batches because the API accepts maximum 5\n",
    "            # documents per request to get embeddings\n",
    "            batch, remaining = (\n",
    "                # batch contains the first `num_instances_per_batch` documents\n",
    "                remaining[: self.num_instances_per_batch],\n",
    "                # docs contains the remaining documents\n",
    "                remaining[self.num_instances_per_batch :],\n",
    "            )\n",
    "            self.vertex_ai_embeddings = VertexAIEmbeddings()\n",
    "            chunk = self.vertex_ai_embeddings.client.get_embeddings(batch)\n",
    "            results.extend(chunk)\n",
    "            next(limiter)\n",
    "\n",
    "        return [r.values for r in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a foundational LLM instance\n",
    "# LLM model\n",
    "llm = VertexAI(\n",
    "    model_name=\"text-bison@002\",\n",
    "    max_output_tokens=2048,\n",
    "    # setting the temperature that low will make the mode more deterministic\n",
    "    temperature=0.5,\n",
    "    # (nucleus sampling): the model will only consider a cumulative probability threshold above 0.8 to consider words to use\n",
    "    top_p=0.8,\n",
    "    # the model will only consider the top k most likely words when generating text\n",
    "    top_k=40,\n",
    "    verbose=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set embeddings\n",
    "# `QPM` stands for \"queries per minute\"\n",
    "EMBEDDING_QPM = 100\n",
    "EMBEDDING_NUM_BATCH = 5\n",
    "embeddings = CustomVertexAIEmbeddings(\n",
    "    requests_per_minute=EMBEDDING_QPM,\n",
    "    num_instances_per_batch=EMBEDDING_NUM_BATCH,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic interactions with an LLM\n",
    "\n",
    "Text is the natural way of interacting with LLMs, so let's just do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" The future of AI multimodality holds immense potential for advancing various fields and transforming our interactions with technology. Here are some key areas where AI multimodality is expected to make significant impacts:\\n\\n**Enhanced Human-Computer Interaction:** AI multimodality will enable more natural and intuitive interactions between humans and machines. By combining different modalities such as speech, gestures, facial expressions, and touch, AI systems can better understand user intent and provide more personalized and responsive experiences. This will revolutionize user interfaces, making them more user-friendly and accessible.\\n\\n**Improved Accessibility:** AI multimodality can significantly enhance accessibility for individuals with disabilities. By providing alternative input and output methods, such as voice commands, gesture recognition, and haptic feedback, AI systems can make technology more inclusive and accessible to a wider range of users.\\n\\n**Enhanced Healthcare Diagnostics and Treatment:** AI multimodality has the potential to revolutionize healthcare by enabling more accurate and efficient diagnostics. By analyzing multiple types of data, such as medical images, patient records, and sensor data, AI systems can provide doctors with a more comprehensive understanding of a patient's condition and recommend personalized treatment plans.\\n\\n**Advanced Robotics and Autonomous Systems:** AI multimodality is crucial for the development of advanced robotics and autonomous systems. By integrating various sensors and perception modalities, these systems can perceive their surroundings more accurately, make informed decisions, and interact with the physical world more effectively. This will drive advancements in fields such as autonomous vehicles, industrial automation, and space exploration.\\n\\n**Immersive Virtual and Augmented Reality Experiences:** AI multimodality plays a vital role in creating immersive virtual and augmented reality experiences. By combining visual, auditory, haptic, and other sensory modalities, AI systems can generate realistic and engaging virtual environments that cater to different user preferences and enhance the overall user experience.\\n\\n**Personalized Education and Training:** AI multimodality can revolutionize education and training by providing personalized learning experiences. By analyzing individual learning styles, preferences, and progress, AI systems can deliver tailored content, interactive simulations, and feedback, making learning more engaging and effective.\\n\\n**Enhanced Cybersecurity:** AI multimodality can contribute to improved cybersecurity by analyzing multiple data sources and detecting anomalies that may indicate potential threats. By combining information from network traffic, user behavior, and device sensors, AI systems can provide more comprehensive and proactive security measures.\\n\\n**Intelligent Transportation Systems:** AI multimodality is essential for the development of intelligent transportation systems. By integrating data from sensors, cameras, and traffic management systems, AI systems can optimize traffic flow, reduce congestion, and improve overall transportation efficiency and safety.\\n\\n**Advanced Manufacturing and Supply Chain Management:** AI multimodality can optimize manufacturing processes and supply chain management by analyzing data from various sources, such as production lines, inventory levels, and logistics operations. This enables real-time decision-making, predictive maintenance, and improved resource allocation, leading to increased efficiency and productivity.\\n\\nThese are just a few examples of the vast potential of AI multimodality. As AI technology continues to advance and become more sophisticated, we can expect to see even more groundbreaking applications and innovations that transform the way we interact with technology and the world around us.\""
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interaction = \"what is the future of AI multi modality?\"\n",
    "response = llm(interaction)\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Multimodal AI systems will become increasingly sophisticated, enabling them to process and understand a wider range of data types, including text, images, audio, and video. This will lead to a new generation of AI applications that can interact with the world in more natural and intuitive ways.\n"
     ]
    }
   ],
   "source": [
    "chat = ChatVertexAI()\n",
    "\n",
    "initial_human_message = HumanMessage(\n",
    "    content=\"What is the future of AI multi modality?\",\n",
    ")\n",
    "\n",
    "chat_interaction = [\n",
    "    SystemMessage(\n",
    "        content=\"You are an AI assistant that is specialized in software enginering and in machine learning. Your answers are two short sentences long maximum.\",\n",
    "    ),\n",
    "    initial_human_message\n",
    "]\n",
    "\n",
    "response = chat(chat_interaction)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Multimodal AI systems will be able to process and understand a wider range of data types, including text, images, audio, video, and even sensory data such as touch and smell. This will allow them to interact with the world in a more natural and intuitive way.\n"
     ]
    }
   ],
   "source": [
    "response_with_history = chat([\n",
    "    SystemMessage(\n",
    "        content=\"You are an AI assistant that is specialized in software enginering and in machine learning. Your answers are two short sentences long maximum.\",\n",
    "    ),\n",
    "    initial_human_message,\n",
    "    AIMessage(\n",
    "        content=response.content,\n",
    "    ),\n",
    "    HumanMessage(content=\"what wider range of data types?\")\n",
    "])\n",
    "\n",
    "print(response_with_history.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
