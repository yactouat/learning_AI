{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain X PaLM\n",
    "\n",
    "This notebook contains the code for the LangChain + PaLM blog articles available on https://yactouat.com/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting things up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from google.cloud import aiplatform\n",
    "import langchain\n",
    "from langchain.chat_models import ChatVertexAI\n",
    "from langchain.embeddings import VertexAIEmbeddings\n",
    "from langchain.llms import VertexAI\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "# supporting type hints\n",
    "from typing import List\n",
    "import vertexai\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertex AI SDK version: 1.37.0\n",
      "LangChain version: 0.0.346\n"
     ]
    }
   ],
   "source": [
    "# `GCP_LOCATION` is for instance `europe-west1`\n",
    "vertexai.init(project=os.getenv('GCP_PROJECT_ID'), location=os.getenv('GCP_LOCATION')) \n",
    "\n",
    "print(f\"Vertex AI SDK version: {aiplatform.__version__}\")\n",
    "\n",
    "print(f\"LangChain version: {langchain.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function to rate limit API calls\n",
    "def rate_limit(max_per_minute):\n",
    "    period = 60 / max_per_minute\n",
    "    print(\"Waiting\")\n",
    "    while True:\n",
    "        before = time.time()\n",
    "        # the `yield` keyword here controls back to the caller;\n",
    "        # e.g. it allows the caller to perform the action that is rate limited\n",
    "        yield\n",
    "        # at this points, control returns to this function\n",
    "        after = time.time()\n",
    "        elapsed = after - before\n",
    "        sleep_time = max(0, period - elapsed)\n",
    "        if sleep_time > 0:\n",
    "            print(\".\", end=\"\")\n",
    "            time.sleep(sleep_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomVertexAIEmbeddings():\n",
    "    def __init__(self, requests_per_minute: int, num_instances_per_batch: int):\n",
    "        self.requests_per_minute = requests_per_minute\n",
    "        self.num_instances_per_batch = num_instances_per_batch\n",
    "\n",
    "    # takes a list of strings and returns a list of embeddings\n",
    "    def embed_documents(self, texts: List[str]):\n",
    "        limiter = rate_limit(self.requests_per_minute)\n",
    "        results = []\n",
    "        remaining = list(texts)\n",
    "\n",
    "        while remaining:\n",
    "            # working in batches because the API accepts maximum 5\n",
    "            # documents per request to get embeddings\n",
    "            batch, remaining = (\n",
    "                # batch contains the first `num_instances_per_batch` documents\n",
    "                remaining[: self.num_instances_per_batch],\n",
    "                # docs contains the remaining documents\n",
    "                remaining[self.num_instances_per_batch :],\n",
    "            )\n",
    "            self.vertex_ai_embeddings = VertexAIEmbeddings()\n",
    "            chunk = self.vertex_ai_embeddings.client.get_embeddings(batch)\n",
    "            results.extend(chunk)\n",
    "            next(limiter)\n",
    "\n",
    "        return [r.values for r in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a foundational LLM instance\n",
    "# LLM model\n",
    "llm = VertexAI(\n",
    "    model_name=\"text-bison-32k@002\",\n",
    "    max_output_tokens=8192,\n",
    "    # setting the temperature that low will make the mode more deterministic\n",
    "    temperature=0.5,\n",
    "    # (nucleus sampling): the model will only consider a cumulative probability threshold above 0.8 to consider words to use\n",
    "    top_p=0.8,\n",
    "    # the model will only consider the top k most likely words when generating text\n",
    "    top_k=40,\n",
    "    verbose=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set embeddings\n",
    "# `QPM` stands for \"queries per minute\"\n",
    "EMBEDDING_QPM = 100\n",
    "EMBEDDING_NUM_BATCH = 5\n",
    "embeddings = CustomVertexAIEmbeddings(\n",
    "    requests_per_minute=EMBEDDING_QPM,\n",
    "    num_instances_per_batch=EMBEDDING_NUM_BATCH,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic interactions with an LLM\n",
    "\n",
    "Text is the natural way of interacting with LLMs, so let's just do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" The future of AI multimodality is bright, with many exciting possibilities for how it can be used to improve our lives. Here are a few ways that AI multimodality could be used in the future:\\n\\n**1. Personalized experiences:** AI multimodality can be used to create personalized experiences for users, by combining data from multiple sources to create a more complete picture of their needs and preferences. For example, an AI multimodal system could use data from a user's social media activity, purchase history, and location to recommend products and services that they are likely to be interested in.\\n\\n**2. Improved healthcare:** AI multimodality can be used to improve healthcare by providing doctors with more information about their patients. For example, an AI multimodal system could combine data from a patient's medical records, imaging scans, and genetic information to help doctors diagnose diseases and develop treatment plans.\\n\\n**3. Enhanced education:** AI multimodality can be used to enhance education by providing students with more engaging and interactive learning experiences. For example, an AI multimodal system could combine text, images, videos, and audio to create immersive learning environments that help students learn more effectively.\\n\\n**4. Safer transportation:** AI multimodality can be used to make transportation safer by providing drivers with more information about their surroundings. For example, an AI multimodal system could combine data from cameras, sensors, and GPS to create a real-time map of the road that shows drivers where there are hazards.\\n\\n**5. More efficient manufacturing:** AI multimodality can be used to improve manufacturing efficiency by automating tasks and optimizing processes. For example, an AI multimodal system could combine data from sensors, cameras, and robots to identify inefficiencies in the manufacturing process and make adjustments to improve productivity.\\n\\nThese are just a few examples of the many ways that AI multimodality could be used in the future. As AI technology continues to develop, we can expect to see even more innovative and groundbreaking applications of AI multimodality in the years to come.\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interaction = \"what is the future of AI multi modality?\"\n",
    "response = llm(interaction)\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Multimodal AI systems will become increasingly sophisticated, enabling them to process and understand a wider range of data types, including text, images, audio, and video. This will allow them to perform a broader range of tasks, such as generating creative content, providing personalized recommendations, and making complex decisions.\n"
     ]
    }
   ],
   "source": [
    "chat = ChatVertexAI(model_name=\"chat-bison-32k@002\")\n",
    "\n",
    "initial_human_message = HumanMessage(\n",
    "    content=\"What is the future of AI multi modality?\",\n",
    ")\n",
    "\n",
    "chat_interaction = [\n",
    "    SystemMessage(\n",
    "        content=\"You are an AI assistant that is specialized in software enginering and in machine learning. Your answers are two short sentences long maximum.\",\n",
    "    ),\n",
    "    initial_human_message\n",
    "]\n",
    "\n",
    "response = chat(chat_interaction)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Multimodal AI systems will be able to process and understand a wider range of data types, including text, images, audio, video, and even haptic data. This will allow them to interact with the world in a more natural and intuitive way.\n"
     ]
    }
   ],
   "source": [
    "response_with_history = chat([\n",
    "    SystemMessage(\n",
    "        content=\"You are an AI assistant that is specialized in software enginering and in machine learning. Your answers are two short sentences long maximum.\",\n",
    "    ),\n",
    "    initial_human_message,\n",
    "    AIMessage(\n",
    "        content=response.content,\n",
    "    ),\n",
    "    HumanMessage(content=\"what wider range of data types?\")\n",
    "])\n",
    "\n",
    "print(response_with_history.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[-0.033491309732198715,\n",
       "  -0.024705076590180397,\n",
       "  0.04814573749899864,\n",
       "  -0.0027620443142950535,\n",
       "  -0.006553742568939924],\n",
       " [-0.011842082254588604,\n",
       "  0.0037548220716416836,\n",
       "  -0.010157657787203789,\n",
       "  -0.009990965016186237,\n",
       "  0.03311667591333389],\n",
       " [0.018089881166815758,\n",
       "  -0.012067587114870548,\n",
       "  0.02881603129208088,\n",
       "  0.05261128023266792,\n",
       "  -0.003405238501727581]]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here's how you can get an embedding for a sentence\n",
    "text = \"I love Shakespeare\"\n",
    "text2 = \"Life without theaters would be dull\"\n",
    "# using a completely unrelated sentence\n",
    "text3 = \"Hey honey where did you put the keys?\"\n",
    "\n",
    "# sending the text to create embeddings for to Vertex AI\n",
    "text_embeddings = embeddings.embed_documents([text, text2, text3])\n",
    "# showing only the first 5 values of each embedding\n",
    "text_embeddings_short = [e[:5] for e in text_embeddings]\n",
    "text_embeddings_short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9161093776749432, 1.0209345760221844)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the cosine similarity between the first two embeddings\n",
    "euclidian_distance_1_2 = np.linalg.norm(np.array(text_embeddings[0]) - np.array(text_embeddings[1]))\n",
    "# now between the first and the third\n",
    "euclidian_distance_1_3 = np.linalg.norm(np.array(text_embeddings[0]) - np.array(text_embeddings[2]))\n",
    "\n",
    "euclidian_distance_1_2, euclidian_distance_1_3\n",
    "# we can see that the first two embeddings are closer to each other than the first and the third\n",
    "# ! watch out, this is just an example as the magnitude of the embeddings is not normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5803716847710505, 0.4788461710635313)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now let's do the same thing using cosine similarity\n",
    "cosine_similarity_1_2 = np.dot(text_embeddings[0], text_embeddings[1]) / (\n",
    "    np.linalg.norm(text_embeddings[0]) * np.linalg.norm(text_embeddings[1])\n",
    ")\n",
    "cosine_similarity_1_3 = np.dot(text_embeddings[0], text_embeddings[2]) / (\n",
    "    np.linalg.norm(text_embeddings[0]) * np.linalg.norm(text_embeddings[2])\n",
    ")\n",
    "\n",
    "cosine_similarity_1_2, cosine_similarity_1_3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
