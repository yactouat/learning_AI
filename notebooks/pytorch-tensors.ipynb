{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading and setting up dependencies\n",
    "import fastbook\n",
    "\n",
    "fastbook.setup_book()\n",
    "\n",
    "from fastbook import *\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `PyTorch` tensors\n",
    "\n",
    "PyTorch tensors are multidimensional arrays that can be used for numerical computations.\n",
    "\n",
    "`NumPy` arrays are multidimensional tables of data, with all items of the same type. `PyTorch` tensors are different in that they have to use a single basic numeric type for all components of the array. Also `PyTorch` tensors cannot be _jagged_, e.g. containing arrays of different sizes in it.\n",
    "\n",
    "One of the main features of `PyTorch` tensors, compared to `NumPy` arrays, is that they can be used on a GPU to accelerate computing. They are also able to compute gradients by keeping track of successive operations using the calculus chain rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To create an array or tensor, pass a list (or list of lists, or list of lists of lists, etc.) to `array()` or `tensor()`:\n",
    "data = [[1,2,3],[4,5,6]]\n",
    "arr = array(data) # `NumPy`\n",
    "tns = tensor(data) # `PyTorch`\n",
    "\n",
    "tns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to select a row of a tensor\n",
    "tns[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 5])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to select a column of a tensor,\n",
    "# `:` here says \"take all the values of axis 0\"\n",
    "tns[:, 1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 6])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combination of the above using `slice` syntax\n",
    "tns[1,1:3]\n",
    "\n",
    "# this says \"take the second row, then take columns 1 to 3 (not including 3)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 3, 4],\n",
       "        [5, 6, 7]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `PyTorch` makes it very easy to perform arithmetic operations at scale\n",
    "tns+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.LongTensor'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all `PyTorch` tensors have a type\n",
    "tns.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.5000, 3.0000, 4.5000],\n",
       "        [6.0000, 7.5000, 9.0000]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this type can change automatically based on the operations you perform\n",
    "tns*1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute derivatives with `PyTorch`\n",
    "\n",
    "Consider the statement below:\n",
    "\n",
    "```python\n",
    "xt = tensor(3.).requires_grad_()\n",
    "```\n",
    "\n",
    "This statement does 2 main things:\n",
    "\n",
    "- creates a tensor `xt` with the value `3.`\n",
    "- it enables the gradient computation with `requires_grad_()`: this has the effect of modfying the tensor so that `PyTorch` keeps track of the operations performed directly on it using a computation graph, this feature being later used to calculate the gradients (_automatic differentiation_)\n",
    "\n",
    "Let's see this in action with a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3., requires_grad=True),\n",
       " tensor(6.),\n",
       " tensor(9., grad_fn=<PowBackward0>))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating our tensor and enabling gradients computation\n",
    "xt = torch.tensor(3.).requires_grad_()\n",
    "\n",
    "# perform operations on the tensor using the example of a simple quadratic function\n",
    "yt = xt**2\n",
    "\n",
    "## let's compute the derivative of `yt` with respect to `xt` (backward propagation)\n",
    "yt.backward()\n",
    "\n",
    "# let's see the values of `xt`, its derivative (`gradient` in deep learning jargon), and `yt`;\n",
    "# here, `xt.grad` is the derivative of `yt` with respect to `xt`\n",
    "xt, xt.grad, yt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `backward` method above refers to _backpropagation_, which is the process of calculating the derivative of each layer.\n",
    "\n",
    "Let's do the same thing with more values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.,  4., 10.], requires_grad=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xt = tensor([3.,4.,10.]).requires_grad_()\n",
    "xt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(125., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# taking in a rank-1 tensor and returning a tensor with a scalar value (rank-0 tensor)\n",
    "def f(x): return (x**2).sum()\n",
    "\n",
    "yt = f(xt)\n",
    "yt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6.,  8., 20.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's get the gradients\n",
    "yt.backward()\n",
    "xt.grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
